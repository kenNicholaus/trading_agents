{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    import pandas as pd\n",
    "    import pandas_datareader.data as web\n",
    "    import pickle \n",
    "    import requests\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    from collections import deque\n",
    "    import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Curiosity Q-learning agent'\n",
    "image_path = 'D:/kenneth/agent/images/'\n",
    "path = 'D:/kenneth/trading/'\n",
    "run_date=open(path+'run_date.txt').read()\n",
    "start=('1970-01-01')\n",
    "end=('2019-12-31')\n",
    "days=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(image_path+name):\n",
    "        os.makedirs(image_path+name) \n",
    "with open('D:/kenneth/trading/sp500/sp500tickers.txt','r') as f:\n",
    "    tickers=[line.rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    LEARNING_RATE = 0.003\n",
    "    BATCH_SIZE = 64\n",
    "    LAYER_SIZE = 512\n",
    "    OUTPUT_SIZE = 3\n",
    "    EPSILON = 0.5\n",
    "    DECAY_RATE = 0.005\n",
    "    MIN_EPSILON = 0.1\n",
    "    GAMMA = 0.99\n",
    "    MEMORIES = deque()\n",
    "    COPY = 1000\n",
    "    T_COPY = 0\n",
    "    MEMORY_SIZE = 300\n",
    "    \n",
    "    def __init__(self, state_size, window_size, trend, skip):\n",
    "        self.state_size = state_size\n",
    "        self.window_size = window_size\n",
    "        self.half_window = window_size // 2\n",
    "        self.trend = trend\n",
    "        self.skip = skip\n",
    "        tf.reset_default_graph()\n",
    "        self.X = tf.placeholder(tf.float32, (None, self.state_size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, self.state_size))\n",
    "        self.ACTION = tf.placeholder(tf.float32, (None))\n",
    "        self.REWARD = tf.placeholder(tf.float32, (None))\n",
    "        self.batch_size = tf.shape(self.ACTION)[0]\n",
    "        \n",
    "        with tf.variable_scope('curiosity_model'):\n",
    "            action = tf.reshape(self.ACTION, (-1,1))\n",
    "            state_action = tf.concat([self.X, action], axis=1)\n",
    "            save_state = tf.identity(self.Y)\n",
    "            \n",
    "            feed = tf.layers.dense(state_action, 32, activation=tf.nn.relu)\n",
    "            self.curiosity_logits = tf.layers.dense(feed, self.state_size)\n",
    "            self.curiosity_cost = tf.reduce_sum(tf.square(save_state - self.curiosity_logits), axis=1)\n",
    "            \n",
    "            self.curiosity_optimizer = tf.train.RMSPropOptimizer(self.LEARNING_RATE)\\\n",
    "            .minimize(tf.reduce_mean(self.curiosity_cost))\n",
    "        \n",
    "        total_reward = tf.add(self.curiosity_cost, self.REWARD)\n",
    "        \n",
    "        with tf.variable_scope(\"q_model\"):\n",
    "            with tf.variable_scope(\"eval_net\"):\n",
    "                x_action = tf.layers.dense(self.X, 128, tf.nn.relu)\n",
    "                self.logits = tf.layers.dense(x_action, self.OUTPUT_SIZE)\n",
    "            \n",
    "            with tf.variable_scope(\"target_net\"):\n",
    "                y_action = tf.layers.dense(self.Y, 128, tf.nn.relu)\n",
    "                y_q = tf.layers.dense(y_action, self.OUTPUT_SIZE)\n",
    "            \n",
    "            q_target = total_reward + self.GAMMA * tf.reduce_max(y_q, axis=1)\n",
    "            action = tf.cast(self.ACTION, tf.int32)\n",
    "            action_indices = tf.stack([tf.range(self.batch_size, dtype=tf.int32), action], axis=1)\n",
    "            q = tf.gather_nd(params=self.logits, indices=action_indices)\n",
    "            self.cost = tf.losses.mean_squared_error(labels=q_target, predictions=q)\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.LEARNING_RATE).minimize(\n",
    "            self.cost, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"q_model/eval_net\"))\n",
    "            \n",
    "        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_model/target_net')\n",
    "        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_model/eval_net')\n",
    "        self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _memorize(self, state, action, reward, new_state, done):\n",
    "        self.MEMORIES.append((state, action, reward, new_state, done))\n",
    "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
    "            self.MEMORIES.popleft()\n",
    "            \n",
    "    def get_state(self, t):\n",
    "        window_size = self.window_size + 1\n",
    "        d = t - window_size + 1\n",
    "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
    "        res = []\n",
    "        for i in range(window_size - 1):\n",
    "            res.append(block[i + 1] - block[i])\n",
    "        return np.array(res)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.logits, feed_dict={self.X:inputs})\n",
    "    \n",
    "    def get_predicted_action(self, sequence):\n",
    "        prediction = self.predict(np.array(sequence))[0]\n",
    "        return np.argmax(prediction)\n",
    "    \n",
    "    def _select_action(self, state):\n",
    "        if np.random.rand() < self.EPSILON:\n",
    "            action = np.random.randint(self.OUTPUT_SIZE)\n",
    "        else:\n",
    "            action = self.get_predicted_action([state])\n",
    "        return action\n",
    "    \n",
    "    def _construct_memories(self, replay):\n",
    "        states = np.array([a[0] for a in replay])\n",
    "        actions = np.array([a[1] for a in replay])\n",
    "        rewards = np.array([a[2] for a in replay])\n",
    "        new_states = np.array([a[3] for a in replay])\n",
    "        if (self.T_COPY + 1) % self.COPY == 0:\n",
    "            self.sess.run(self.target_replace_op)\n",
    "            \n",
    "        cost, _ = self.sess.run([self.cost, self.optimizer], feed_dict = {\n",
    "            self.X: states, self.Y: new_states, self.ACTION: actions, self.REWARD: rewards\n",
    "        })\n",
    "        \n",
    "        if (self.T_COPY + 1) % self.COPY == 0:\n",
    "            self.sess.run(self.curiosity_optimizer, feed_dict = {\n",
    "                self.X: states, self.Y: new_states, self.ACTION: actions, self.REWARD: rewards\n",
    "            })\n",
    "        return cost\n",
    "    \n",
    "    def buy(self, initial_money):\n",
    "        starting_money = initial_money\n",
    "        states_sell = []\n",
    "        states_buy = []\n",
    "        inventory = []\n",
    "        state = self.get_state(0)\n",
    "        for t in range(0, len(self.trend) - 1, self.skip):\n",
    "            action = self._select_action(state)\n",
    "            next_state = self.get_state(t + 1)\n",
    "            \n",
    "            if action == 1 and initial_money >= self.trend[t]:\n",
    "                inventory.append(self.trend[t])\n",
    "                initial_money -= self.trend[t]\n",
    "                states_buy.append(t)\n",
    "#                 print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
    "            \n",
    "            elif action == 2 and len(inventory):\n",
    "                bought_price = inventory.pop(0)\n",
    "                initial_money += self.trend[t]\n",
    "                states_sell.append(t)\n",
    "                try:\n",
    "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
    "                except:\n",
    "                    invest = 0\n",
    "#                 print(\n",
    "#                     'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
    "#                     % (t, close[t], invest, initial_money)\n",
    "#                 )\n",
    "            \n",
    "            state = next_state\n",
    "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
    "        total_gains = initial_money - starting_money\n",
    "        #self.sess.close()\n",
    "        print(total_gains, invest)\n",
    "        return states_buy, states_sell, total_gains, invest\n",
    "        \n",
    "    def train(self, iterations, checkpoint, initial_money):\n",
    "        for i in range(iterations):\n",
    "            total_profit = 0\n",
    "            inventory = []\n",
    "            state = self.get_state(0)\n",
    "            starting_money = initial_money\n",
    "            for t in range(0, len(self.trend) - 1, self.skip):\n",
    "                \n",
    "                action = self._select_action(state)\n",
    "                next_state = self.get_state(t + 1)\n",
    "                \n",
    "                if action == 1 and starting_money >= self.trend[t]:\n",
    "                    inventory.append(self.trend[t])\n",
    "                    starting_money -= self.trend[t]\n",
    "                \n",
    "                elif action == 2 and len(inventory) > 0:\n",
    "                    bought_price = inventory.pop(0)\n",
    "                    total_profit += self.trend[t] - bought_price\n",
    "                    starting_money += self.trend[t]\n",
    "                    \n",
    "                invest = ((starting_money - initial_money) / initial_money)\n",
    "                \n",
    "                self._memorize(state, action, invest, next_state, starting_money < initial_money)\n",
    "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
    "                state = next_state\n",
    "                replay = random.sample(self.MEMORIES, batch_size)\n",
    "                cost = self._construct_memories(replay)\n",
    "                self.T_COPY += 1\n",
    "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
    "            if (i+1) % checkpoint == 0:\n",
    "                pass\n",
    "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
    "                                                                                  starting_money))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(ticker):\n",
    "    close = df['Close']\n",
    "    fig = plt.figure(figsize = (15,5), facecolor='yellowgreen', dpi=100)\n",
    "    plt.plot(close, color='k', lw=3., alpha=0.5)\n",
    "    plt.plot(close, '^', markersize=10, color='g', label = 'buying signal', markevery = states_buy)\n",
    "    plt.plot(close, 'v', markersize=10, color='r', label = 'selling signal', markevery = states_sell)\n",
    "    plt.title(name+'\\n'+'Ticker: '+ticker)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.gca().axes.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(image_path+name+'/'+ticker+'.png', facecolor='yellowgreen', dpi=100)\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_only(days):\n",
    "    new_buy = []\n",
    "    new_sell = []\n",
    "    display_days = days\n",
    "    total_days= df.shape[0]\n",
    "    display_from = total_days-display_days\n",
    "    for i in(states_buy):\n",
    "        if i > display_from:\n",
    "            adj = i- display_from\n",
    "            new_buy.append(adj)\n",
    "    for i in(states_sell):\n",
    "        if i > display_from:\n",
    "            adj = i- display_from\n",
    "            new_sell.append(adj)\n",
    "    \n",
    "    return(new_buy, new_sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_days(days, ticker):\n",
    "    close = df['Close'][-days:]\n",
    "    fig = plt.figure(figsize = (15,5), facecolor='yellowgreen', dpi=100)\n",
    "    plt.plot(close, color='k', lw=3., alpha=0.5)\n",
    "    plt.plot(close, '^', markersize=12, color='g',alpha=1.0,label = 'buying signal', markevery = new_buy)\n",
    "    plt.plot(close, 'v', markersize=12, color='r',alpha=1.0, label = 'selling signal', markevery = new_sell)\n",
    "    plt.title(name+'\\n'+'Last '+str(days)+' Daily Trade Recommendations')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.gca().axes.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(image_path+name+'/'+ticker+'_'+str(days)+'.png', facecolor='yellowgreen', dpi=100)\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started ticker: A\n",
      "epoch: 10, total rewards: -97.654266.3, cost: 10805.411133, total money: 9362.585732\n",
      "epoch: 20, total rewards: 129.815069.3, cost: 15472.955078, total money: 9623.485075\n",
      "epoch: 30, total rewards: 875.078300.3, cost: 19925.556641, total money: 10306.458298\n",
      "epoch: 40, total rewards: 1024.087399.3, cost: 37153.109375, total money: 9144.797410\n",
      "epoch: 50, total rewards: 981.461094.3, cost: 32411.644531, total money: 9487.721096\n",
      "epoch: 60, total rewards: 393.130892.3, cost: 31184.822266, total money: 9755.720892\n",
      "epoch: 70, total rewards: 1791.522837.3, cost: 34469.128906, total money: 8077.007721\n",
      "epoch: 80, total rewards: 1567.683390.3, cost: 31248.537109, total money: 7694.829475\n",
      "epoch: 90, total rewards: 1900.342114.3, cost: 39983.574219, total money: 7938.038524\n",
      "epoch: 100, total rewards: 202.118238.3, cost: 43895.882812, total money: 10202.118238\n",
      "epoch: 110, total rewards: 182.362093.3, cost: 36190.796875, total money: 9824.862101\n",
      "epoch: 120, total rewards: 466.110521.3, cost: 22453.242188, total money: 9715.610529\n",
      "epoch: 130, total rewards: 1736.876255.3, cost: 37110.437500, total money: 8201.224161\n",
      "epoch: 140, total rewards: 1568.206419.3, cost: 30051.855469, total money: 10066.806406\n",
      "epoch: 150, total rewards: 1285.680746.3, cost: 21384.927734, total money: 6747.286816\n",
      "epoch: 160, total rewards: 1021.900399.3, cost: 12115.834961, total money: 7882.808146\n",
      "epoch: 170, total rewards: 1251.242499.3, cost: 12268.333984, total money: 7270.721030\n",
      "epoch: 180, total rewards: 1055.089445.3, cost: 17008.164062, total money: 8943.519442\n",
      "epoch: 190, total rewards: 814.144176.3, cost: 22764.865234, total money: 8618.554172\n",
      "epoch: 200, total rewards: 750.261497.3, cost: 20406.601562, total money: 9710.091503\n",
      "epoch: 210, total rewards: 1231.393218.3, cost: 8534.640625, total money: 9089.883216\n",
      "epoch: 220, total rewards: 1227.454713.3, cost: 10372.386719, total money: 8228.194711\n",
      "epoch: 230, total rewards: 509.650005.3, cost: 16244.529297, total money: 8806.819996\n",
      "epoch: 240, total rewards: 995.144258.3, cost: 7609.894531, total money: 6889.841232\n",
      "epoch: 250, total rewards: 1056.036432.3, cost: 6218.500000, total money: 8378.626413\n",
      "epoch: 260, total rewards: 354.834112.3, cost: 4073.469727, total money: 8551.144114\n",
      "epoch: 270, total rewards: 387.616339.3, cost: 9457.969727, total money: 8711.156336\n",
      "epoch: 280, total rewards: 1696.877390.3, cost: 7269.069824, total money: 9041.147394\n",
      "epoch: 290, total rewards: 652.563824.3, cost: 6397.727539, total money: 9477.933819\n",
      "epoch: 300, total rewards: 778.879276.3, cost: 13846.383789, total money: 7440.073765\n",
      "epoch: 310, total rewards: 1055.266695.3, cost: 10410.660156, total money: 8219.756689\n",
      "epoch: 320, total rewards: 1336.599702.3, cost: 16404.269531, total money: 7260.809241\n",
      "epoch: 330, total rewards: 651.344646.3, cost: 12094.559570, total money: 8397.064651\n",
      "epoch: 340, total rewards: 899.501209.3, cost: 20044.009766, total money: 9404.621216\n",
      "epoch: 350, total rewards: 378.091671.3, cost: 32104.734375, total money: 7808.531677\n",
      "epoch: 360, total rewards: 1049.890534.3, cost: 14327.352539, total money: 6520.114779\n",
      "epoch: 370, total rewards: 652.698762.3, cost: 19899.597656, total money: 7766.758759\n",
      "epoch: 380, total rewards: 592.516593.3, cost: 6389.364746, total money: 8784.586593\n",
      "epoch: 390, total rewards: 975.352607.3, cost: 7475.605469, total money: 8236.082610\n",
      "epoch: 400, total rewards: 1096.438162.3, cost: 5601.567383, total money: 7830.058157\n",
      "epoch: 410, total rewards: 1010.660401.3, cost: 8284.612305, total money: 10318.060403\n",
      "epoch: 420, total rewards: 823.863117.3, cost: 5538.219238, total money: 10237.033119\n",
      "epoch: 430, total rewards: 669.482832.3, cost: 5532.434082, total money: 9768.352827\n",
      "epoch: 440, total rewards: 424.706769.3, cost: 6105.959961, total money: 8885.186772\n",
      "epoch: 450, total rewards: 393.661035.3, cost: 3882.549805, total money: 8431.361035\n",
      "epoch: 460, total rewards: 328.555538.3, cost: 3044.617188, total money: 7837.739456\n",
      "epoch: 470, total rewards: 983.416695.3, cost: 1276.149536, total money: 9780.446697\n",
      "epoch: 480, total rewards: 357.947628.3, cost: 2170.031738, total money: 8563.777637\n",
      "epoch: 490, total rewards: 624.080472.3, cost: 3243.956055, total money: 9699.140473\n",
      "epoch: 500, total rewards: 908.481090.3, cost: 48403.710938, total money: 8845.064174\n",
      "epoch: 510, total rewards: 726.076016.3, cost: 24696.087891, total money: 7796.169431\n",
      "epoch: 520, total rewards: 742.682856.3, cost: 30362.878906, total money: 9948.442865\n",
      "epoch: 530, total rewards: 570.562403.3, cost: 28219.492188, total money: 9171.942411\n",
      "epoch: 540, total rewards: 631.328874.3, cost: 33513.003906, total money: 9085.108876\n",
      "epoch: 550, total rewards: 545.033007.3, cost: 7419.499512, total money: 10469.153009\n",
      "epoch: 560, total rewards: 300.053956.3, cost: 6878.506836, total money: 8541.833966\n",
      "epoch: 570, total rewards: 704.457177.3, cost: 3550.077637, total money: 9753.787190\n",
      "epoch: 580, total rewards: 1505.051782.3, cost: 6553.668457, total money: 10058.111787\n",
      "epoch: 590, total rewards: 338.926521.3, cost: 13171.715820, total money: 10338.926521\n",
      "epoch: 600, total rewards: 967.055279.3, cost: 8057.431152, total money: 10310.725281\n",
      "epoch: 610, total rewards: -194.075604.3, cost: 15747.045898, total money: 9510.484401\n",
      "epoch: 620, total rewards: 285.168553.3, cost: 6327.321289, total money: 9621.668557\n",
      "epoch: 630, total rewards: 1108.079246.3, cost: 11836.842773, total money: 10362.629245\n",
      "epoch: 640, total rewards: 956.654576.3, cost: 3657.961914, total money: 9323.834573\n",
      "epoch: 650, total rewards: 399.971310.3, cost: 3697.933350, total money: 9876.161312\n",
      "epoch: 660, total rewards: 54.604200.3, cost: 4397.675781, total money: 9542.354208\n",
      "epoch: 670, total rewards: 137.899340.3, cost: 5038.452637, total money: 10059.929338\n",
      "epoch: 680, total rewards: 92.631832.3, cost: 4828.606445, total money: 10092.631832\n",
      "epoch: 690, total rewards: 9.789363.3, cost: 2951.929199, total money: 9794.569369\n",
      "epoch: 700, total rewards: 1113.958045.3, cost: 1641.118652, total money: 10064.768050\n",
      "epoch: 710, total rewards: 43.901781.3, cost: 6839.998047, total money: 10043.901781\n",
      "epoch: 720, total rewards: 546.253199.3, cost: 2176.425293, total money: 9958.953203\n",
      "epoch: 730, total rewards: 2016.660473.3, cost: 2100.781738, total money: 9534.385197\n",
      "epoch: 740, total rewards: 686.168722.3, cost: 1522.841064, total money: 10225.388723\n",
      "epoch: 750, total rewards: 254.588467.3, cost: 3692.372803, total money: 10254.588467\n",
      "epoch: 760, total rewards: 1349.384841.3, cost: 546.323608, total money: 6506.219897\n",
      "epoch: 770, total rewards: 191.386116.3, cost: 1092.525879, total money: 9905.296120\n",
      "epoch: 780, total rewards: 165.314391.3, cost: 537.624756, total money: 8156.584384\n",
      "epoch: 790, total rewards: 407.591012.3, cost: 500.250763, total money: 9002.681008\n",
      "epoch: 800, total rewards: 160.900266.3, cost: 362.505798, total money: 10160.900266\n",
      "epoch: 810, total rewards: 211.482260.3, cost: 250.404266, total money: 9507.472258\n",
      "epoch: 820, total rewards: 204.171397.3, cost: 232.230286, total money: 6989.811393\n",
      "epoch: 830, total rewards: 929.073786.3, cost: 230.376129, total money: 8085.707331\n",
      "epoch: 840, total rewards: 534.547130.3, cost: 338.440369, total money: 8795.867129\n",
      "epoch: 850, total rewards: 204.479336.3, cost: 684.031128, total money: 7876.779335\n",
      "epoch: 860, total rewards: 1464.554505.3, cost: 309.311646, total money: 9414.424489\n",
      "epoch: 870, total rewards: 1998.388638.3, cost: 370.034119, total money: 9138.343154\n",
      "epoch: 880, total rewards: 342.317100.3, cost: 238.780823, total money: 8145.587096\n",
      "epoch: 890, total rewards: 199.899817.3, cost: 1482.746582, total money: 10199.899817\n",
      "epoch: 900, total rewards: -102.203696.3, cost: 1765.432251, total money: 9828.306306\n",
      "epoch: 910, total rewards: 138.698378.3, cost: 1007.347351, total money: 10138.698378\n",
      "epoch: 920, total rewards: 71.797040.3, cost: 5049.755859, total money: 9999.657041\n",
      "epoch: 930, total rewards: 14.610720.3, cost: 550.612244, total money: 9862.070719\n",
      "epoch: 940, total rewards: 1127.291767.3, cost: 234.916153, total money: 5966.969940\n",
      "epoch: 950, total rewards: 614.937965.3, cost: 154.823990, total money: 9453.077957\n",
      "epoch: 960, total rewards: 416.389254.3, cost: 326.485168, total money: 6544.299253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 970, total rewards: 465.822825.3, cost: 1418.490967, total money: 9012.102824\n",
      "epoch: 980, total rewards: 6.451119.3, cost: 489.345947, total money: 9931.881120\n",
      "epoch: 990, total rewards: 71.039209.3, cost: 803.403015, total money: 9437.699205\n",
      "epoch: 1000, total rewards: 894.402475.3, cost: 41410.820312, total money: 7273.612474\n",
      "-2251.407588005066 -22.51407588005066\n",
      "Started ticker: AAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, total rewards: -84.780006.3, cost: 8.950980, total money: 9461.479995\n",
      "epoch: 20, total rewards: 51.519997.3, cost: 83.428406, total money: 9867.339995\n",
      "epoch: 30, total rewards: -249.350003.3, cost: 30.028088, total money: 9385.849998\n",
      "epoch: 40, total rewards: -291.479978.3, cost: 218.140350, total money: 8965.900023\n",
      "epoch: 50, total rewards: 966.580019.3, cost: 117.649658, total money: 9552.530018\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-278da45e96c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m               skip = skip)\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_money\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_money\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-166c5d12aa00>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, iterations, checkpoint, initial_money)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mreplay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMEMORIES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m                 \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_memories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT_COPY\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPSILON\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMIN_EPSILON\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMIN_EPSILON\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDECAY_RATE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-166c5d12aa00>\u001b[0m in \u001b[0;36m_construct_memories\u001b[1;34m(self, replay)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         cost, _ = self.sess.run([self.cost, self.optimizer], feed_dict = {\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACTION\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREWARD\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         })\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_money = 10000\n",
    "window_size = 30\n",
    "skip = 10\n",
    "iterations = 1000\n",
    "checkpoint = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = pd.read_csv(path+'sp500/'+run_date+'/'+ticker+'.csv', index_col=0, parse_dates=True)\n",
    "    df =df[start:end]\n",
    "    print('Started ticker:', ticker)\n",
    "\n",
    "    close = df.Close.values.tolist()\n",
    "\n",
    "    agent = Agent(state_size = window_size, \n",
    "              window_size = window_size, \n",
    "              trend = close, \n",
    "              skip = skip)\n",
    "    \n",
    "    agent.train(iterations = iterations, checkpoint = checkpoint, initial_money = initial_money)\n",
    "    \n",
    "\n",
    "    states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)\n",
    "    \n",
    "#     while total_gains<100:\n",
    "#         agent.train(iterations = iterations, checkpoint = checkpoint, initial_money = initial_money)\n",
    "#         states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)\n",
    "#         print(total_gains)\n",
    "    plot_all(ticker)\n",
    "#     new_buy, new_sell = display_only(days)\n",
    "#     plot_by_days(days, ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu]",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
